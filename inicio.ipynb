{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df955ac",
   "metadata": {},
   "source": [
    "# Funcion para hacer el scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b839a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOQUE 1: Importaciones y configuraci√≥n inicial\n",
    "\n",
    "import requests # para hacer peticiones HTTP (descargar paginas web)\n",
    "from bs4 import BeautifulSoup # para analizar el HTML descargado\n",
    "import sqlite3 \n",
    "import pandas as pd \n",
    "import time \n",
    "import re # para expresiones regulares (extraer numeros de texto)\n",
    "from urllib.parse import urljoin, quote # urljoin: Construir URLs completas a partir de relativas \n",
    "import json # para trabajar con JSON \n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")\n",
    "print(\"üï∑Ô∏è Comenzando la infiltraci√≥n en Books To Scrape...\") \n",
    "\n",
    "\n",
    "# BLOQUE 2: Funciones de Web Scraping \n",
    "\n",
    "def get_all_categories ():\n",
    "    ''' OBTIENE TODAS LAS CATEGORIAS DEL SITIO'''\n",
    "    url = \"https://books.toscrape.com\" \n",
    "    respuesta = requests.get(url) # se descarga la web \n",
    "    soup = BeautifulSoup(respuesta.content, 'html.parser')  \n",
    "    # convierte texto HTML ilegible, en un objeto inteligente que se pueda navegar \n",
    "    \n",
    "    categories = []\n",
    "    nav_list = soup.find('ul', class_='nav nav-list')\n",
    "    if nav_list:\n",
    "        category_links = nav_list.find_all ('a') [1:] # Saltar \"Books\"\n",
    "        for link in category_links:\n",
    "            category_name = link.text.strip()  # Extrae el texto del enlace y quita espacios \n",
    "            category_url = urljoin(url, link['href'])  # Convierte la URL relativa a absoluta \n",
    "            categories.append({ \n",
    "                'name' : category_name,\n",
    "                'url' : category_url \n",
    "            })\n",
    "            \n",
    "    print (f\"üéØ Encontradas {len(categories)} categorias\")\n",
    "    return categories \n",
    "\n",
    "\n",
    "def book_quantity (book_url): \n",
    "    ''' OBTIENE LA CANTIDAD EN STOCK DE UN LIBRO DESDE SU PAGINA DE DETALLES  '''\n",
    "    try:\n",
    "        soup_quantity = BeautifulSoup(requests.get(book_url).content,'html.parser') # descarga y parsea la pagina individual del libro \n",
    "        quantity_text = soup_quantity.select_one('p.instock.availability').get_text(strip=True) \n",
    "        # Busca el <p> con clases \"instock\" y \"availability\", extrae su texto limpio\n",
    "        match = re.search(r'\\((\\d+)\\)', quantity_text) # busca un patron para extraer el numero entero \n",
    "        if match:\n",
    "            return int (match.group(1)) # devuelve la cantidad encontrada \n",
    "        else:\n",
    "            return 0 # si no se encuentra la cantidad, devuelve 0 \n",
    "    except Exception as e :\n",
    "        print (f\"‚ùå Erorr obteniendo cantidad para {book_url}:{e}\")\n",
    "        return 0 # en caso de error, devuelve 0 \n",
    "    \n",
    "\n",
    "\n",
    "def scrape_books_from_page(page_url): \n",
    "    ''' SCRAPE LIBROS DE UNA PAGINA ESPECIFICA  '''\n",
    "    response = requests.get(page_url) # Descarga la pagina \n",
    "    soup = BeautifulSoup(response.content,'html.parser' ) # Parsea el HTML \n",
    "    \n",
    "    books = []\n",
    "    book_containers = soup.find_all ('article', class_ ='product_pod') \n",
    "    # cada libro esta dentro de un <article class=\"product_pod\"> \n",
    "    \n",
    "    for book in book_containers: \n",
    "        try:\n",
    "            # TITULO\n",
    "            title_element = book.find('h3').find('a') # busca la a dentro del h3 \n",
    "            title = title_element['title'] # el titulo completo esta en el atributo \"title\"\n",
    "\n",
    "            # URL DEL LIBRO PARA MAS DETALLES \n",
    "            book_url = urljoin(page_url, title_element['href']) \n",
    "            # cosntruye la url absoluta del libro a partir de su href relativo\n",
    "            \n",
    "            \n",
    "            # PRECIO\n",
    "            price_element = book.find('p', class_= 'price_color') \n",
    "            price_text = price_element.text.strip() if price_element else \"¬£0.00\"\n",
    "            price = float (price_text.lstrip('√Ç¬£')) # Elimina los caracteres \"√Ç\" y \"¬£\" del inicio y convierte a n√∫mero decimal\n",
    "            \n",
    "            # RATING \n",
    "            rating_element = book.find ('p', class_= 'star-rating')\n",
    "            rating_class = rating_element['class'][1] if rating_element else 'Zero' \n",
    "            # La clase CSS indica el rating\n",
    "            rating_map = {'One': 1, 'Two' : 2 , 'Three': 3, 'Four': 4, 'Five': 5, \"Zero\" : 0 } \n",
    "            rating = rating_map.get(rating_class, 0) \n",
    "            # Convierte la palabra en ingl√©s a n√∫mero \n",
    "            \n",
    "            # STOCK \n",
    "            stock_element = book.find ('p', class_= 'instock availability')\n",
    "            in_stock = 'In stock' in stock_element.text if stock_element else False \n",
    "            # Verifica si el texto contiene \"In stock\" ‚Üí True/False\n",
    "            quantity = book_quantity(book_url)\n",
    "            # Llama a la funci√≥n anterior para obtener la cantidad exacta en stock\n",
    "            # ‚ö†Ô∏è NOTA: Esto hace una petici√≥n HTTP extra POR CADA LIBRO (lento)\n",
    "            \n",
    "            books.append({ # se agrega todos los datos del libro al diccionario \n",
    "                \n",
    "                'title': title,\n",
    "                'price': price,\n",
    "                'rating' : rating,\n",
    "                'in_stock' : in_stock,\n",
    "                'quantity': quantity,\n",
    "                'url': book_url\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error procesando libro: {e}\") \n",
    "            continue\n",
    "    return books \n",
    "\n",
    "def scrape_all_books (): \n",
    "    ''' SCRAPE TODOS LOS LIBROS DEL SITIO '''\n",
    "    all_books = []\n",
    "    categories = get_all_categories() # obtiene la lista de categorias\n",
    "    for i, category in enumerate(categories): \n",
    "        print (f\"Procesando categoria {i+1}/ {len(categories)}: {category['name']}\")\n",
    "        \n",
    "        page_num = 1 \n",
    "        current_url = category['url'] # URL de la primera pagina de la categoria \n",
    "        \n",
    "        while current_url: # sigue el bucle mientras haya paginas\n",
    "            print (f\" Pagina {page_num}\")\n",
    "            books_on_page = scrape_books_from_page(current_url) \n",
    "            # Extare todos los libros de la pagina actual \n",
    "            \n",
    "            for book in books_on_page:\n",
    "                book['category'] = category['name']\n",
    "                # agrega el nombre de la categoria a cada libro \n",
    "                \n",
    "            all_books.extend(books_on_page) # se agrega los libros a la lista total \n",
    "            \n",
    "            # BUSCAR SIGUIENTE PAGINA \n",
    "            response = requests.get(current_url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            next_button = soup.find('li', class_= 'next')\n",
    "            # busca el boton next para ir a la siguiente pagina \n",
    "            \n",
    "            if next_button and next_button.find('a'):\n",
    "                next_url = next_button.find('a')['href']\n",
    "                current_url = urljoin(current_url, next_url)\n",
    "                # construye la URL a la siguiente pagina \n",
    "                page_num += 1 \n",
    "            else:\n",
    "                current_url = None # no hay mas paginas y sale del while \n",
    "                \n",
    "            time.sleep (0.5) # ser amigables con el servidor, espera 0.5 entre peticiones \n",
    "        \n",
    "    print (f\" üéâ Scraping completado: {len(all_books)} libros encontrados\")\n",
    "    return all_books\n",
    "\n",
    "\n",
    "# BLOQUE 3: EJECUTAR EL SCRAPING \n",
    "books_data = scrape_all_books\n",
    "\n",
    "# Mostrar muestra de datos \n",
    "print (\"\\n Muestra de los primeros 3 libros: \")\n",
    "for i, book in enumerate(books_data[:3]):\n",
    "    print(f\"{i+1}. {book['title']} - {book['price']} - ‚≠ê{book['rating']} - {book['category']} - {book['quantity']}\")\n",
    "    # se imprime los primeros 3 libros como muestra "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e7e784",
   "metadata": {},
   "source": [
    "## Funciones para obtener autores con Google Books API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687225ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_from_google_books (title, max_retries):\n",
    "    ''' Obtiene la informacion del autor usando Google Books API '''\n",
    "    \n",
    "    for attempt in range (max_retries):\n",
    "        try:\n",
    "            # Limpiar el t√≠tulo para mejor b√∫squeda\n",
    "            clean_title = re.sub(r'[^\\w\\s]', '', title) \n",
    "            url = \"https://www.googleapis.com/books/v1/volumes\" # url de la base de Google Books API \n",
    "            params = {\"q\": f\"intitle:{title}\", \"maxResults\": 1} \n",
    "            # q: intitle:{title} ‚Äî busca libros cuyo t√≠tulo contenga el texto dado.\n",
    "            # maxResults: 1 ‚Äî solo pide 1 resultado (el m√°s relevante).\n",
    "            response = requests.get(url, params=params) \n",
    "            # hace la petici√≥n HTTP GET a la API. requests se encarga de construir la URL final con los par√°metros\n",
    "            \n",
    "            if response.status_code == 200: # verifica que la respuesta fue exitosa, y es 200 porque es un estandar del protocolo HTTP\n",
    "                data = response.json() \n",
    "                # Convierte la respuesta (que viene en formato JSON) a un diccionario de Python.\n",
    "\n",
    "                if 'items' in data and len(data['items']) > 0:\n",
    "                    # Verifica que la API devolvi√≥ resultados. La clave 'items' contiene la lista de libros encontrados.\n",
    "                    \n",
    "                    volume_info = data['items'][0].get('volumeInfo', {}) \n",
    "                    #Toma el primer resultado ([0]) y extrae su informaci√≥n del volumen. Si no existe 'volumeInfo', devuelve un diccionario vac√≠o {} para evitar errores.\n",
    "                    authors = volume_info.get('authors', ['Autor Desconocido']) # extrae la lista de autores \n",
    "                    description = volume_info.get('description', 'Sin descripci√≥n') # extrae la descripcion del libro \n",
    "\n",
    "                    return {\n",
    "                        'authors': authors,\n",
    "                        'description': description[:500] + '...' if len(description) > 500 else description\n",
    "                        # Si la descripci√≥n tiene m√°s de 500 caracteres, la recorta y le agrega '...' al final.\n",
    "                    }\n",
    "                else:\n",
    "                    return {\n",
    "                        'authors': ['Autor Desconocido'],\n",
    "                        'description': 'Sin descripci√≥n'\n",
    "                        # si la API no encontro ningun libro, devuelve valores por defecto \n",
    "                    }\n",
    "\n",
    "            else:\n",
    "                time.sleep(1)  # espera y reintenta \n",
    "                continue\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error consultando API para '{title}': {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            # si aun quedan reintentos, se espera 2 segundos y vuelve a intentar \n",
    "            else:\n",
    "                \n",
    "                return {\n",
    "                    'authors': ['Autor Desconocido'],\n",
    "                    'description': 'Sin descripci√≥n'\n",
    "            # si se agotaron devuelve los valores por defecto \n",
    "                }\n",
    "    \n",
    "    # Si llegamos aqu√≠, todos los intentos fallaron\n",
    "    return {\n",
    "        'authors': ['Autor Desconocido'],\n",
    "        'description': 'Sin descripci√≥n'\n",
    "    }\n",
    "\n",
    "def enrich_books_with_authors(books_list): # recibe la lista completa que se obtuvo del scraping \n",
    "    \"\"\"Enriquece la lista de libros con informaci√≥n de autores\"\"\"\n",
    "    print(\"üîç Consultando Google Books API para obtener autores...\")\n",
    "    \n",
    "    enriched_books = []\n",
    "    total_books = len(books_list) \n",
    "    # Crea una lista vac√≠a para los libros enriquecidos y guarda el total de libros para mostrar progreso.\n",
    "\n",
    "    for i, book in enumerate(books_list):\n",
    "        print(f\"üìñ Procesando libro {i+1}/{total_books}: {book['title'][:50]}...\")\n",
    "        # imprime progreso actual y los primeros 50 caracteres del titulo \n",
    "        \n",
    "        author_info = get_author_from_google_books(book['title']) \n",
    "        # se llama a la funcion de la API para buscar el autor del libro en Google Books API \n",
    "        \n",
    "        enriched_book = book.copy() # se crea para no modificar la original \n",
    "        enriched_book.update(author_info) # Fusiona la info del autor (authors y description) dentro del diccionario del libro\n",
    "        enriched_books.append(enriched_book) # y se agrega el libro enriquecido a la lista final  \n",
    "        \n",
    "        # Rate limiting - ser respetuosos con la API\n",
    "        time.sleep(0.1) # para no saturar la API\n",
    "        \n",
    "        # Mostrar progreso cada 10 libros\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  ‚úÖ Progreso: {i+1}/{total_books} libros procesados\")\n",
    "    \n",
    "    print(\"üéä Enriquecimiento completado!\")\n",
    "    return enriched_books\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# BLOQUE 5: Ejecutar enriquecimiento con API\n",
    "print(\"üåü Enriqueciendo datos con informaci√≥n de autores...\")\n",
    "enriched_books = enrich_books_with_authors(books_data) # se le pasa los libros del scraping \n",
    "\n",
    "# Mostrar muestra de datos enriquecidos\n",
    "print(\"\\nüìä Muestra de libros enriquecidos:\")\n",
    "for i, book in enumerate(enriched_books[:3]):\n",
    "    authors_str = ', '.join(book['authors']) \n",
    "    print(f\"{i+1}. '{book['title']}' por {authors_str}\")\n",
    "    print(f\"   Precio: {book['price']} | Rating: ‚≠ê{book['rating']} | Categor√≠a: {book['category']}\")\n",
    "    # Resumen de cada libro \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fa5dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BLOQUE 6: Dise√±o UML y Estructura de Base de Datos\n",
    "\n",
    "\n",
    "print(\"\"\"\n",
    "üèóÔ∏è DIAGRAMA UML - Estructura de la Base de Datos\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ     AUTORES     ‚îÇ    ‚îÇ  LIBROS_AUTORES  ‚îÇ    ‚îÇ     LIBROS      ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îÇ   (Tabla Pivot)  ‚îÇ    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ id (PK)         ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚î§ autor_id (FK)    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ id (PK)         ‚îÇ\n",
    "‚îÇ nombre          ‚îÇ    ‚îÇ libro_id (FK)    ‚îÇ    ‚îÇ titulo          ‚îÇ\n",
    "‚îÇ                 ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ precio          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                           ‚îÇ rating          ‚îÇ\n",
    "                                              ‚îÇ categoria       ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                           ‚îÇ en_stock        ‚îÇ\n",
    "|                 |                           | cantidad        |\n",
    "‚îÇ   CATEGORIAS    ‚îÇ                           ‚îÇ                 ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                           ‚îÇ descripcion     ‚îÇ\n",
    "‚îÇ id (PK)         ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ categoria_id (FK)‚îÇ\n",
    "‚îÇ nombre          ‚îÇ                           ‚îÇ url             ‚îÇ\n",
    "‚îÇ                 ‚îÇ                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Relaciones:\n",
    "- AUTORES ‚Üî LIBROS: Muchos a Muchos (un autor puede escribir varios libros, \n",
    "  un libro puede tener varios autores)\n",
    "- CATEGORIAS ‚Üî LIBROS: Uno a Muchos (una categor√≠a tiene muchos libros)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
