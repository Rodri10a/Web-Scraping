{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df955ac",
   "metadata": {},
   "source": [
    "# Funcion para hacer el scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b839a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOQUE 1: Importaciones y configuraci√≥n inicial\n",
    "\n",
    "import requests # para hacer peticiones HTTP (descargar paginas web)\n",
    "from bs4 import BeautifulSoup # para analizar el HTML descargado\n",
    "import sqlite3 \n",
    "import pandas as pd \n",
    "import time \n",
    "import re # para expresiones regulares (extraer numeros de texto)\n",
    "from urllib.parse import urljoin, quote # urljoin: Construir URLs completas a partir de relativas \n",
    "import json # para trabajar con JSON \n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")\n",
    "print(\"üï∑Ô∏è Comenzando la infiltraci√≥n en Books To Scrape...\") \n",
    "\n",
    "\n",
    "# BLOQUE 2: Funciones de Web Scraping \n",
    "\n",
    "def get_all_categories ():\n",
    "    ''' OBTIENE TODAS LAS CATEGORIAS DEL SITIO'''\n",
    "    url = \"https://books.toscrape.com\" \n",
    "    respuesta = requests.get(url) # se descarga la web \n",
    "    soup = BeautifulSoup(respuesta.content, 'html.parser')  \n",
    "    # convierte texto HTML ilegible, en un objeto inteligente que se pueda navegar \n",
    "    \n",
    "    categories = []\n",
    "    nav_list = soup.find('ul', class_='nav nav-list')\n",
    "    if nav_list:\n",
    "        category_links = nav_list.find_all ('a') [1:] # Saltar \"Books\"\n",
    "        for link in category_links:\n",
    "            category_name = link.text.strip()  # Extrae el texto del enlace y quita espacios \n",
    "            category_url = urljoin(url, link['href'])  # Convierte la URL relativa a absoluta \n",
    "            categories.append({ \n",
    "                'name' : category_name,\n",
    "                'url' : category_url \n",
    "            })\n",
    "            \n",
    "    print (f\"üéØ Encontradas {len(categories)} categorias\")\n",
    "    return categories \n",
    "\n",
    "\n",
    "def book_quantity (book_url): \n",
    "    ''' OBTIENE LA CANTIDAD EN STOCK DE UN LIBRO DESDE SU PAGINA DE DETALLES  '''\n",
    "    try:\n",
    "        soup_quantity = BeautifulSoup(requests.get(book_url).content,'html.parser') # descarga y parsea la pagina individual del libro \n",
    "        quantity_text = soup_quantity.select_one('p.instock.availability').get_text(strip=True) \n",
    "        # Busca el <p> con clases \"instock\" y \"availability\", extrae su texto limpio\n",
    "        match = re.search(r'\\((\\d+)\\)', quantity_text) # busca un patron para extraer el numero entero \n",
    "        if match:\n",
    "            return int (match.group(1)) # devuelve la cantidad encontrada \n",
    "        else:\n",
    "            return 0 # si no se encuentra la cantidad, devuelve 0 \n",
    "    except Exception as e :\n",
    "        print (f\"‚ùå Erorr obteniendo cantidad para {book_url}:{e}\")\n",
    "        return 0 # en caso de error, devuelve 0 \n",
    "    \n",
    "\n",
    "\n",
    "def scrape_books_from_page(page_url): \n",
    "    ''' SCRAPE LIBROS DE UNA PAGINA ESPECIFICA  '''\n",
    "    response = requests.get(page_url) # Descarga la pagina \n",
    "    soup = BeautifulSoup(response.content,'html.parser' ) # Parsea el HTML \n",
    "    \n",
    "    books = []\n",
    "    book_containers = soup.find_all ('article', class_ ='product_pod') \n",
    "    # cada libro esta dentro de un <article class=\"product_pod\"> \n",
    "    \n",
    "    for book in book_containers: \n",
    "        try:\n",
    "            # TITULO\n",
    "            title_element = book.find('h3').find('a') # busca la a dentro del h3 \n",
    "            title = title_element['title'] # el titulo completo esta en el atributo \"title\"\n",
    "\n",
    "            # URL DEL LIBRO PARA MAS DETALLES \n",
    "            book_url = urljoin(page_url, title_element['href']) \n",
    "            # cosntruye la url absoluta del libro a partir de su href relativo\n",
    "            \n",
    "            \n",
    "            # PRECIO\n",
    "            price_element = book.find('p', class_= 'price_color') \n",
    "            price_text = price_element.text.strip() if price_element else \"¬£0.00\"\n",
    "            price = float (price_text.lstrip('√Ç¬£')) # Elimina los caracteres \"√Ç\" y \"¬£\" del inicio y convierte a n√∫mero decimal\n",
    "            \n",
    "            # RATING \n",
    "            rating_element = book.find ('p', class_= 'star-rating')\n",
    "            rating_class = rating_element['class'][1] if rating_element else 'Zero' \n",
    "            # La clase CSS indica el rating\n",
    "            rating_map = {'One': 1, 'Two' : 2 , 'Three': 3, 'Four': 4, 'Five': 5, \"Zero\" : 0 } \n",
    "            rating = rating_map.get(rating_class, 0) \n",
    "            # Convierte la palabra en ingl√©s a n√∫mero \n",
    "            \n",
    "            # STOCK \n",
    "            stock_element = book.find ('p', class_= 'instock availability')\n",
    "            in_stock = 'In stock' in stock_element.text if stock_element else False \n",
    "            # Verifica si el texto contiene \"In stock\" ‚Üí True/False\n",
    "            quantity = book_quantity(book_url)\n",
    "            # Llama a la funci√≥n anterior para obtener la cantidad exacta en stock\n",
    "            # ‚ö†Ô∏è NOTA: Esto hace una petici√≥n HTTP extra POR CADA LIBRO (lento)\n",
    "            \n",
    "            books.append({ # se agrega todos los datos del libro al diccionario \n",
    "                \n",
    "                'title': title,\n",
    "                'price': price,\n",
    "                'rating' : rating,\n",
    "                'in_stock' : in_stock,\n",
    "                'quantity': quantity,\n",
    "                'url': book_url\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error procesando libro: {e}\") \n",
    "            continue\n",
    "    return books \n",
    "\n",
    "def scrape_all_books (): \n",
    "    ''' SCRAPE TODOS LOS LIBROS DEL SITIO '''\n",
    "    all_books = []\n",
    "    categories = get_all_categories() # obtiene la lista de categorias\n",
    "    for i, category in enumerate(categories): \n",
    "        print (f\"Procesando categoria {i+1}/ {len(categories)}: {category['name']}\")\n",
    "        \n",
    "        page_num = 1 \n",
    "        current_url = category['url'] # URL de la primera pagina de la categoria \n",
    "        \n",
    "        while current_url: # sigue el bucle mientras haya paginas\n",
    "            print (f\" Pagina {page_num}\")\n",
    "            books_on_page = scrape_books_from_page(current_url) \n",
    "            # Extare todos los libros de la pagina actual \n",
    "            \n",
    "            for book in books_on_page:\n",
    "                book['category'] = category['name']\n",
    "                # agrega el nombre de la categoria a cada libro \n",
    "                \n",
    "            all_books.extend(books_on_page) # se agrega los libros a la lista total \n",
    "            \n",
    "            # BUSCAR SIGUIENTE PAGINA \n",
    "            response = requests.get(current_url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            next_button = soup.find('li', class_= 'next')\n",
    "            # busca el boton next para ir a la siguiente pagina \n",
    "            \n",
    "            if next_button and next_button.find('a'):\n",
    "                next_url = next_button.find('a')['href']\n",
    "                current_url = urljoin(current_url, next_url)\n",
    "                # construye la URL a la siguiente pagina \n",
    "                page_num += 1 \n",
    "            else:\n",
    "                current_url = None # no hay mas paginas y sale del while \n",
    "                \n",
    "            time.sleep (0.5) # ser amigables con el servidor, espera 0.5 entre peticiones \n",
    "        \n",
    "    print (f\" üéâ Scraping completado: {len(all_books)} libros encontrados\")\n",
    "    return all_books\n",
    "\n",
    "\n",
    "# BLOQUE 3: EJECUTAR EL SCRAPING \n",
    "books_data = scrape_all_books\n",
    "\n",
    "# Mostrar muestra de datos \n",
    "print (\"\\n Muestra de los primeros 3 libros: \")\n",
    "for i, book in enumerate(books_data[:3]):\n",
    "    print(f\"{i+1}. {book['title']} - {book['price']} - ‚≠ê{book['rating']} - {book['category']} - {book['quantity']}\")\n",
    "    # se imprime los primeros 3 libros como muestra "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
